{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaJ8cfXoQ7pvyls34CJBpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siliconshells/Explainable-AI/blob/main/week_6_explainable_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leonard Eshun\n",
        "### Explainable Deep Learning\n",
        "\n",
        "This notebook demonstrates the application of various explainability techniques on a pre-trained ResNet-50 model using images from the ImageNet dataset. The techniques showcased include Grad-CAM, Grad-CAM++, and Score-CAM. ResNet is a widely used convolutional neural network architecture known for its deep layers and skip connections, which help mitigate the vanishing gradient problem. It is one of the most popular and capable architectures for image classification tasks, including butterfly classification.\n",
        "\n",
        "### Motivation\n",
        "Some butterflies are indicators of the health of an ecosystem. They are also important pollinators, and a food source for other animals in the food chain. This makes them important for maintaining biodiversity. Finally, butterflies are simply beautiful creatures that bring joy to many people.     \n",
        "This assignment helps to understand how different CAM explainability techniques can be applied to deep learning models, and how they can provide insights into model predictions. In this case, the ResNet-50 Classifier is used to classify images of butterflies, and the explainability techniques help to visualize which parts of the images are most important for the model's predictions. Overall, it helps to know how to better train and explain deep learning models for image classification tasks."
      ],
      "metadata": {
        "id": "NNIeP2oM0Yw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam --quiet"
      ],
      "metadata": {
        "id": "NIKj_95-066n",
        "outputId": "34802b28-d23d-4909-daad-ec109c759271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.7/7.8 MB\u001b[0m \u001b[31m232.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Import Libraries"
      ],
      "metadata": {
        "id": "LzUT09Zu0fmX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HelG0Rao0CRU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Model\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# CAM utilities\n",
        "from pytorch_grad_cam import (\n",
        "    GradCAM,\n",
        "    ScoreCAM,\n",
        "    HiResCAM,\n",
        ")\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Modeling and Explainability"
      ],
      "metadata": {
        "id": "ZIoWFct01kXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "I used ChatGPT 5 to generate the code for one explanation (the Grad-CAM) with one image\n",
        "on 15-Oct-2025 8:33 AM and I built up on it to generate the other two GradCAM variants after trying multiple variants.\n",
        "I also built the section responsible for plotting the images after trying multiple images with varying results.\n",
        "\"\"\"\n",
        "# Using weights if available\n",
        "try:\n",
        "    from torchvision.models import ResNet50_Weights\n",
        "\n",
        "    try:\n",
        "        model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).eval()\n",
        "    except Exception:\n",
        "        model = resnet50(weights=ResNet50_Weights.DEFAULT).eval()\n",
        "\n",
        "    try:\n",
        "        categories = ResNet50_Weights.IMAGENET1K_V2.meta[\"categories\"]\n",
        "    except Exception:\n",
        "        try:\n",
        "            categories = ResNet50_Weights.DEFAULT.meta[\"categories\"]\n",
        "        except Exception:\n",
        "            categories = None\n",
        "except Exception:\n",
        "    model = resnet50(pretrained=True).eval()\n",
        "    categories = None\n",
        "\n",
        "# Manual, version-safe preprocess for the model\n",
        "preprocess = T.Compose(\n",
        "    [\n",
        "        T.Resize(256),\n",
        "        T.CenterCrop(224),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Visualization transform: Keeping PIL -> numpy in [0,1]\n",
        "vis_transform = T.Compose(\n",
        "    [\n",
        "        T.Resize(256),\n",
        "        T.CenterCrop(224),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(7, 4, figsize=(16, 35))\n",
        "\n",
        "butterfly_types = [\n",
        "    \"Monarch\",\n",
        "    \"Garden Tiger Moth\",\n",
        "    \"Lycaenid\",\n",
        "    \"Ringlet\",\n",
        "    \"Lacewing\",\n",
        "    \"Peacock\",\n",
        "    \"Red Admiral\",\n",
        "]\n",
        "predicted_types = []\n",
        "subplot_titles = []\n",
        "\n",
        "for row in range(7):\n",
        "    # Loading the image\n",
        "    img_path = f\"butterfly{row}.png\"\n",
        "    pil_img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Building overlay base (same spatial size as model input)\n",
        "    vis_pil = vis_transform(pil_img)\n",
        "    rgb_vis = np.array(vis_pil).astype(np.float32) / 255.0  # HxWx3 in [0,1]\n",
        "\n",
        "    # Model input tensor\n",
        "    input_tensor = preprocess(pil_img).unsqueeze(0)\n",
        "\n",
        "    # Predicting the class to explain\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "    probs = torch.softmax(logits, dim=1)[0]\n",
        "    pred_idx = int(torch.argmax(probs))\n",
        "    pred_name = categories[pred_idx] if categories else f\"class_{pred_idx}\"\n",
        "    print(f\"Explaining predicted class: {pred_name} (idx {pred_idx})\")\n",
        "\n",
        "    targets = [ClassifierOutputTarget(pred_idx)]\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    target_layers = [model.layer4[-1]]\n",
        "\n",
        "    # Grad-CAM\n",
        "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
        "        cam_map = cam(input_tensor=input_tensor, targets=targets)[0]  # 224x224, [0,1]\n",
        "        gradcam_vis = show_cam_on_image(rgb_vis, cam_map, use_rgb=True)\n",
        "\n",
        "    # HiRes-CAM\n",
        "    with HiResCAM(model=model, target_layers=target_layers) as campp:\n",
        "        campp_map = campp(input_tensor=input_tensor, targets=targets)[0]\n",
        "        gradcampp_vis = show_cam_on_image(rgb_vis, campp_map, use_rgb=True)\n",
        "\n",
        "    # Score-CAM\n",
        "    with ScoreCAM(model=model, target_layers=target_layers) as sc:\n",
        "        sc_map = sc(input_tensor=input_tensor, targets=targets)[0]\n",
        "        scorecam_vis = show_cam_on_image(rgb_vis, sc_map, use_rgb=True)\n",
        "\n",
        "    axes[row, 0].imshow(vis_pil)\n",
        "    axes[row, 0].set_title(\"Preprocessed view\")\n",
        "    axes[row, 0].axis(\"off\")\n",
        "    axes[row, 1].imshow(gradcam_vis)\n",
        "    axes[row, 1].set_title(\"Grad-CAM\")\n",
        "    axes[row, 1].axis(\"off\")\n",
        "    axes[row, 2].imshow(gradcampp_vis)\n",
        "    axes[row, 2].set_title(\"HiRes-CAM\")\n",
        "    axes[row, 2].axis(\"off\")\n",
        "    axes[row, 3].imshow(scorecam_vis)\n",
        "    axes[row, 3].set_title(\"Score-CAM\")\n",
        "    axes[row, 3].axis(\"off\")\n",
        "\n",
        "    top_5_predictions = \"\"\n",
        "    top5 = torch.topk(probs, k=3)\n",
        "\n",
        "    for p, cls_idx in zip(top5.values.tolist(), top5.indices.tolist()):\n",
        "        name = categories[cls_idx] if categories else f\"class_{cls_idx}\"\n",
        "        top_5_predictions += f\"{name}:{p:.4f}, \"\n",
        "\n",
        "    subplot_titles.append(top_5_predictions)\n",
        "    predicted_types.append(\n",
        "        f\"Explanation {row + 1}. Actual: {butterfly_types[row]}. Predicted: {pred_name.capitalize()}.\"\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Adding central row titles\n",
        "for row in range(7):\n",
        "    fig.text(\n",
        "        0.5,\n",
        "        axes[row, 0].get_position().y1 + 0.02,\n",
        "        predicted_types[row],\n",
        "        ha=\"center\",\n",
        "        va=\"top\",\n",
        "        fontsize=13,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "    fig.text(\n",
        "        0.5,\n",
        "        axes[row, 0].get_position().y1 + 0.014,\n",
        "        f\"Top 3 predictions and scores ({subplot_titles[row].strip().strip(',')})\",\n",
        "        ha=\"center\",\n",
        "        va=\"top\",\n",
        "        fontsize=13,\n",
        "        fontweight=\"regular\",\n",
        "    )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CygJsDsR1hf9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}